{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install globox","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:29:44.862260Z","iopub.execute_input":"2022-11-12T01:29:44.862645Z","iopub.status.idle":"2022-11-12T01:29:56.791041Z","shell.execute_reply.started":"2022-11-12T01:29:44.862610Z","shell.execute_reply":"2022-11-12T01:29:56.789887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom enum import Enum\nimport json\nimport shutil\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\n\nfrom IPython.display import FileLink\nfrom IPython.display import display, HTML\n\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:08:50.280510Z","iopub.execute_input":"2022-11-12T03:08:50.280869Z","iopub.status.idle":"2022-11-12T03:08:50.287952Z","shell.execute_reply.started":"2022-11-12T03:08:50.280838Z","shell.execute_reply":"2022-11-12T03:08:50.286810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"wandb-key\")\n\nwandb.login(key=wandb_key)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:28:44.912563Z","iopub.execute_input":"2022-11-12T01:28:44.912933Z","iopub.status.idle":"2022-11-12T01:28:48.222358Z","shell.execute_reply.started":"2022-11-12T01:28:44.912902Z","shell.execute_reply":"2022-11-12T01:28:48.221265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create data","metadata":{}},{"cell_type":"code","source":"class MyConfig:\n    sample_subm = pd.read_csv('../input/detection-of-human-silhouettes-in-forest-images/omsk/sample_solution.csv')\n    train = pd.read_csv('../input/detection-of-human-silhouettes-in-forest-images/train_dataset_train/train.csv')\n    test_dir = '../input/detection-of-human-silhouettes-in-forest-images/test_dataset_test/test'\n    train_dir = '../input/detection-of-human-silhouettes-in-forest-images/train_dataset_train/train'\n    train_img_dir = './data/train/images'\n    train_labels_dir = './data/train/labels'\n    val_img_dir = './data/val/images'\n    val_labels_dir = './data/val/labels'\n    make_inference = True\n    train_val_split = True\n    make_training = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MyConfig.sample_subm['region_shape'] = MyConfig.sample_subm['region_shape'].astype('object')","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:28:55.764263Z","iopub.execute_input":"2022-11-12T01:28:55.765229Z","iopub.status.idle":"2022-11-12T01:28:55.775565Z","shell.execute_reply.started":"2022-11-12T01:28:55.765142Z","shell.execute_reply":"2022-11-12T01:28:55.774524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_dirs_train_yolo():\n    shutil.rmtree('./data', ignore_errors=True)\n    Path(\"./data/train/images\").mkdir(parents=True, exist_ok=True)\n    Path(\"./data/train/labels\").mkdir(parents=True, exist_ok=True)\n    Path(\"./data/val/images\").mkdir(parents=True, exist_ok=True)\n    Path(\"./data/val/labels\").mkdir(parents=True, exist_ok=True)\nmake_dirs_train_yolo()","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:28:57.100215Z","iopub.execute_input":"2022-11-12T01:28:57.101190Z","iopub.status.idle":"2022-11-12T01:28:57.109395Z","shell.execute_reply.started":"2022-11-12T01:28:57.101153Z","shell.execute_reply":"2022-11-12T01:28:57.107199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def yolobbox2bbox(x,y,w,h):\n    x1, y1 = x-w/2, y-h/2\n    x2, y2 = x+w/2, y+h/2\n    return x1, y1, x2, y2\n\n# Convert Coco bb to Yolo\ndef coco_to_yolo(x1, y1, w, h, image_w, image_h):\n    return [((2*x1 + w)/(2*image_w)) , ((2*y1 + h)/(2*image_h)), w/image_w, h/image_h]","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:28:59.038218Z","iopub.execute_input":"2022-11-12T01:28:59.038988Z","iopub.status.idle":"2022-11-12T01:28:59.045148Z","shell.execute_reply.started":"2022-11-12T01:28:59.038950Z","shell.execute_reply":"2022-11-12T01:28:59.044096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vis_example(path):\n    img = cv2.imread(path)\n    h, w = img.shape[0], img.shape[1]\n    \n    train = MyConfig.train\n    \n    coords_str = train[train.count_region != 0].iloc[6]['region_shape'].replace(\"'\", \"\")\n    circle_coords = json.loads(coords_str)\n    yolo_coords_f = ''\n    for item in circle_coords:\n        cx = item['cx']\n        cy = item['cy']\n        r = int(item['r'] // 1.5)\n    #     r = item['r']\n        top_left_cornel = (cx - r, cy - r)\n        bottom_right_cornel = (cx + r, cy + r)\n\n        yolo_coords = coco_to_yolo(*top_left_cornel, bottom_right_cornel[0] - top_left_cornel[0], bottom_right_cornel[1] - top_left_cornel[1], w, h)\n        x1, y1 = top_left_cornel[0], top_left_cornel[1]\n        w, h = bottom_right_cornel[0] - top_left_cornel[0], bottom_right_cornel[1] - top_left_cornel[1]\n        img = cv2.rectangle(img, (x1, y1), (x1+w, y1+h), (255,0,0), 2)\n\n        label = '0 ' + ' '.join(list(map(str, yolo_coords))) + '\\n'\n        yolo_coords_f = yolo_coords_f + label\n    plt.figure(figsize=(30, 20))\n    plt.imshow(img)\n    print(yolo_coords_f)\n\nvis_example(f'{MyConfig.train_dir}/4269.JPG')","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:29:00.420300Z","iopub.execute_input":"2022-11-12T01:29:00.422975Z","iopub.status.idle":"2022-11-12T01:29:05.007717Z","shell.execute_reply.started":"2022-11-12T01:29:00.422937Z","shell.execute_reply":"2022-11-12T01:29:05.005391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_val_split(train_on_all_data=True):\n    \n    train = MyConfig.train\n    val_part = pd.concat([train[train.count_region != 0].sample(frac=0.2, random_state=42), train[train.count_region == 0].sample(frac=0.01, random_state=42)])\n    \n    if train_on_all_data:\n        train_part = train\n        train_part = pd.concat([train_part[train_part.count_region != 0], train_part[train_part.count_region == 0].sample(frac=0.03, random_state=42)])\n    else:\n        train_part = train[~train.isin(val_part)].dropna()\n        train_part = pd.concat([train_part[train_part.count_region != 0], train_part[train_part.count_region == 0].sample(frac=0.03, random_state=42)])\n    \n    print(len(train_part), len(train_part[train_part.count_region > 0]), len(val_part), len(val_part[val_part.count_region > 0]))\n    # val_part = pd.concat([train[train.count_region != 0].sample(frac=0.2, random_state=42), train[train.count_region == 0].sample(frac=0.001, random_state=42)])\n    # train_part = train[~train.isin(val_part)].dropna()\n    # train_part = pd.concat([train_part[train_part.count_region != 0], train_part[train_part.count_region == 0].sample(frac=0.003, random_state=42)])\n    return train_part, val_part\n\nif MyConfig.train_val_split:\n    train_part, val_part = train_val_split()","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:35:22.438913Z","iopub.execute_input":"2022-11-12T01:35:22.439369Z","iopub.status.idle":"2022-11-12T01:35:22.467355Z","shell.execute_reply.started":"2022-11-12T01:35:22.439298Z","shell.execute_reply":"2022-11-12T01:35:22.466397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_lables(df ,train_img_dir, train_labels_dir):\n    print('imgs with labels: \\n')\n    for i, row in df.iterrows():\n        shutil.copy(f'{MyConfig.train_dir}/{row[\"ID_img\"]}', train_img_dir)\n        if row['count_region'] == 0:\n            open(f'{train_labels_dir}/{row[\"ID_img\"].split(\".\")[0]}.txt', 'a').close()\n        else:\n            print(f'{train_img_dir}/{row[\"ID_img\"]}')\n            img = cv2.imread(f'{train_img_dir}/{row[\"ID_img\"]}')\n            h, w = img.shape[0], img.shape[1]\n\n            coords_str = row['region_shape'].replace(\"'\", \"\")\n            circle_coords = json.loads(coords_str)\n            yolo_coords_f = ''\n\n            for item in circle_coords:\n                cx = item['cx']\n                cy = item['cy']\n                r = item['r']\n                top_left_cornel = (cx - r, cy - r)\n                bottom_right_cornel = (cx + r, cy + r)\n\n                yolo_coords = coco_to_yolo(*top_left_cornel, bottom_right_cornel[0] - top_left_cornel[0], bottom_right_cornel[1] - top_left_cornel[1], w, h)\n\n                label = '0 ' + ' '.join(list(map(str, yolo_coords))) + '\\n'\n                yolo_coords_f = yolo_coords_f + label\n            with open(f'{train_labels_dir}/{row[\"ID_img\"].split(\".\")[0]}.txt', 'a') as label_txt:\n                label_txt.write(yolo_coords_f)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:29:06.086566Z","iopub.execute_input":"2022-11-12T01:29:06.086936Z","iopub.status.idle":"2022-11-12T01:29:06.098101Z","shell.execute_reply.started":"2022-11-12T01:29:06.086903Z","shell.execute_reply":"2022-11-12T01:29:06.097102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if MyConfig.make_training:\n    create_lables(train_part, MyConfig.train_img_dir, MyConfig.train_labels_dir)\n    create_lables(val_part, MyConfig.val_img_dir, MyConfig.val_labels_dir)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:29:09.124501Z","iopub.execute_input":"2022-11-12T01:29:09.125556Z","iopub.status.idle":"2022-11-12T01:29:09.131287Z","shell.execute_reply.started":"2022-11-12T01:29:09.125506Z","shell.execute_reply":"2022-11-12T01:29:09.129615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert labels for mmdetection","metadata":{}},{"cell_type":"code","source":"# rename JPG to jpg\n\ndef rename_jpg_ext(folder: str):\n    for path, dirs, files in os.walk(folder):\n        for file in files:\n            pieces = list(os.path.splitext(file))\n            if pieces[-1] == '.JPG':\n                pieces[-1] = pieces[-1] = '.jpg'\n                new_name = \"\".join(pieces)\n                os.rename(os.path.join(path, file), os.path.join(path, new_name))\n                \nrename_jpg_ext(\"./data/train/images/\")\nrename_jpg_ext(\"./data/val/images/\")    ","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:30:18.834295Z","iopub.execute_input":"2022-11-12T01:30:18.834979Z","iopub.status.idle":"2022-11-12T01:30:18.842186Z","shell.execute_reply.started":"2022-11-12T01:30:18.834943Z","shell.execute_reply":"2022-11-12T01:30:18.841210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from globox import AnnotationSet\n\ndef yolo_to_coco(image_dir: str, labels_dir: str, coco_ds_name: str, label_to_id):\n    yolo_annotations = AnnotationSet.from_yolo(\n        folder = Path(labels_dir),\n        image_folder = Path(image_dir))\n    annot = yolo_annotations.to_coco( \n        label_to_id= label_to_id,\n        auto_ids = True, verbose=False)\n    \n    for a in annot['annotations']:\n        a['area'] = a['bbox'][2]*a['bbox'][3] \n     \n    annot['categories'] = [{\"supercategory\": \"none\", \"id\": i, \"name\": l} for l, i in label_to_id.items()]\n    \n    with open(coco_ds_name, \"w\") as f:\n            json.dump(annot, fp=f, allow_nan=False)\n\nyolo_to_coco(MyConfig.train_img_dir, MyConfig.train_labels_dir, './train.json', {'human': 0})\nyolo_to_coco(MyConfig.val_img_dir, MyConfig.val_labels_dir, './val.json', {'human': 0})","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:30:21.588656Z","iopub.execute_input":"2022-11-12T01:30:21.589414Z","iopub.status.idle":"2022-11-12T01:30:21.599477Z","shell.execute_reply.started":"2022-11-12T01:30:21.589377Z","shell.execute_reply":"2022-11-12T01:30:21.598242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/working/classes.txt\", \"w\") as classes:\n    classes.write(\"human\")","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:30:24.067858Z","iopub.execute_input":"2022-11-12T01:30:24.068902Z","iopub.status.idle":"2022-11-12T01:30:24.074151Z","shell.execute_reply.started":"2022-11-12T01:30:24.068860Z","shell.execute_reply":"2022-11-12T01:30:24.073186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MMdetection","metadata":{}},{"cell_type":"markdown","source":"### Install","metadata":{}},{"cell_type":"code","source":"!pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torch-1.7.0+cu110-cp37-cp37m-linux_x86_64.whl' --no-deps\n!pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torchvision-0.8.1+cu110-cp37-cp37m-linux_x86_64.whl' --no-deps\n!pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torchaudio-0.7.0-cp37-cp37m-linux_x86_64.whl' --no-deps","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install '/kaggle/input/mmdetectionv2140/addict-2.4.0-py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/yapf-0.31.0-py2.py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/terminal-0.4.0-py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/terminaltables-3.1.0-py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/mmcv_full-1_3_8-cu110-torch1_7_0/mmcv_full-1.3.8-cp37-cp37m-manylinux1_x86_64.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/pycocotools-2.0.2/pycocotools-2.0.2' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/mmpycocotools-12.0.3/mmpycocotools-12.0.3' --no-deps\n\n!rm -rf mmdetection\n\n!cp -r /kaggle/input/mmdetectionv2140/mmdetection-2.14.0 /kaggle/working/\n!mv /kaggle/working/mmdetection-2.14.0 /kaggle/working/mmdetection\n%cd /kaggle/working/mmdetection\n!pip install -e .","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport sklearn\nimport torchvision\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport PIL\nimport json\nfrom PIL import Image, ImageEnhance\nimport albumentations as A\nimport mmdet\nimport mmcv\nfrom albumentations.pytorch import ToTensorV2\nimport seaborn as sns\nimport glob\nfrom pathlib import Path\nimport pycocotools\nfrom pycocotools import mask\nimport numpy.random\nimport random\nimport cv2\nimport re\nfrom mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector\nfrom mmdet.apis import inference_detector, init_detector, show_result_pyplot, set_random_seed","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:32:44.500531Z","iopub.execute_input":"2022-11-12T01:32:44.500864Z","iopub.status.idle":"2022-11-12T01:33:03.789199Z","shell.execute_reply.started":"2022-11-12T01:32:44.500833Z","shell.execute_reply":"2022-11-12T01:33:03.788077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download checkpoint for training","metadata":{}},{"cell_type":"code","source":"!mkdir checkpoints\n!wget -c http://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_rcnn_x101_64x4d_fpn_20e_coco/cascade_rcnn_x101_64x4d_fpn_20e_coco_20200509_224357-051557b1.pth \\\n      -O checkpoints/cascade_rcnn_x101_64x4d_fpn_20e_coco_20200509_224357-051557b1.pth","metadata":{"execution":{"iopub.status.busy":"2022-11-12T00:18:18.896995Z","iopub.execute_input":"2022-11-12T00:18:18.897813Z","iopub.status.idle":"2022-11-12T00:19:14.668852Z","shell.execute_reply.started":"2022-11-12T00:18:18.897775Z","shell.execute_reply":"2022-11-12T00:19:14.667398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"from mmcv import Config\ncfg = Config.fromfile('/kaggle/working/mmdetection/configs/cascade_rcnn/cascade_rcnn_x101_64x4d_fpn_20e_coco.py')","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:33:03.792320Z","iopub.execute_input":"2022-11-12T01:33:03.792674Z","iopub.status.idle":"2022-11-12T01:33:03.832868Z","shell.execute_reply.started":"2022-11-12T01:33:03.792638Z","shell.execute_reply":"2022-11-12T01:33:03.832005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.dataset_type = 'CocoDataset'\ncfg.classes = ['human']\ncfg.data_root = '/kaggle/working'\n\nfor head in cfg.model.roi_head.bbox_head:\n    head.num_classes = 1\n    \n\ncfg.data.test.type = 'CocoDataset'\ncfg.data.test.classes = ('human',)\ncfg.data.test.data_root = '/kaggle/working'\ncfg.data.test.ann_file = './val.json'\ncfg.data.test.img_prefix = 'data/val/images'\n\ncfg.data.train.type = 'CocoDataset'\ncfg.data.train.data_root = '/kaggle/working'\ncfg.data.train.ann_file = './train.json'\ncfg.data.train.img_prefix = 'data/train/images'\ncfg.data.train.classes = ('human',)\n\ncfg.data.val.type = 'CocoDataset'\ncfg.data.val.data_root = '/kaggle/working'\ncfg.data.val.ann_file = './val.json'\ncfg.data.val.img_prefix = 'data/val/images'\ncfg.data.val.classes = ('human',)\n\nalbu_train_transforms = [\n    dict(type='ShiftScaleRotate', shift_limit=0.0625,\n         scale_limit=0.15, rotate_limit=15, p=0.4),\n    dict(type='RandomBrightnessContrast', brightness_limit=0.2,\n         contrast_limit=0.2, p=0.5),\n#     dict(type='IAAAffine', shear=(-10.0, 10.0), p=0.4),\n#     dict(type='CLAHE', p=0.5),\n    dict(\n        type=\"OneOf\",\n        transforms=[\n            dict(type=\"GaussianBlur\", p=1.0, blur_limit=7),\n            dict(type=\"MedianBlur\", p=1.0, blur_limit=7),\n        ],\n        p=0.4,\n    ),\n]\n\ncfg.train_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n#     dict(type='Resize', img_scale=[(440, 596), (480, 650), (520, 704), (580, 785), (620, 839)], multiscale_mode='value', keep_ratio=True),\n#     dict(type='Resize', img_scale=[(880, 1192), (960, 130), (1040, 1408), (1160, 1570), (1240, 1678)], multiscale_mode='value', keep_ratio=True),\n    dict(type='Resize', img_scale=[(1333, 800), (1690, 960)]),\n#     dict(type='Resize', img_scale=(1333, 800)),\n    \n    \n\n    dict(type='RandomFlip', flip_ratio=0.5),\n\n    dict(\n        type='Albu',\n        transforms=albu_train_transforms,\n        bbox_params=dict(\n        type='BboxParams',\n        format='pascal_voc',\n        label_fields=['gt_labels'],\n        min_visibility=0.0,\n        filter_lost_elements=True),\n        keymap=dict(img='image', gt_bboxes='bboxes'),\n        update_pad_shape=False,\n        skip_img_without_anno=True),\n    dict(\n        type='Normalize',\n        mean=[128, 128, 128],\n        std=[11.58, 11.58, 11.58],\n        to_rgb=True),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'), \n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n]\n\ncfg.val_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n#         img_scale=[(880, 1192), (960, 130), (1040, 1408), (1160, 1570), (1240, 1678)],\n        img_scale = [(1333, 800), (1690, 960)],\n#         img_scale=(1333, 800),\n#         img_scale = (520, 704),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(\n                type='Normalize',\n                mean=[128, 128, 128],\n                std=[11.58, 11.58, 11.58],\n                to_rgb=True),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img'])\n        ])\n]\n\n\ncfg.test_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=[(1333, 800), (1690, 960)],\n#         img_scale=(1333, 800),\n        \n#         img_scale = (520, 704),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(\n                type='Normalize',\n                mean=[128, 128, 128],\n                std=[11.58, 11.58, 11.58],\n                to_rgb=True),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img'])\n        ])\n]\n\ncfg.data.train.pipeline = cfg.train_pipeline\ncfg.data.val.pipeline = cfg.val_pipeline\n# cfg.data.test.pipeline = cfg.test_pipeline\n\n\ncfg.data.test.pipeline[1:] = [\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=[(1333, 800), (1690, 960)],\n#         img_scale=(1333, 800),\n        \n#         img_scale = (520, 704),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(\n                type='Normalize',\n                mean=[128, 128, 128],\n                std=[11.58, 11.58, 11.58],\n                to_rgb=True),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img'])\n        ])\n]\n\n\ncfg.model.test_cfg.rcnn.max_per_img = 400\n\ncfg.load_from = './checkpoints/cascade_rcnn_x101_64x4d_fpn_20e_coco_20200509_224357-051557b1.pth'\n\ncfg.work_dir = '/kaggle/working/model_output'\n\ncfg.optimizer.lr = 0.02 / 8\ncfg.lr_config = dict(\n    policy='CosineAnnealing', \n    by_epoch=False,\n    warmup='linear', \n    warmup_iters=125, \n    warmup_ratio=0.001,\n    min_lr=1e-07)\n\ncfg.data.samples_per_gpu = 1\ncfg.data.workers_per_gpu = 0\n\ncfg.evaluation.metric = 'bbox'\ncfg.evaluation.interval = 10\n\ncfg.checkpoint_config.interval = 10\ncfg.runner.max_epochs = 50 # 12\ncfg.log_config.interval = 144\n\n# cfg.model.rpn_head.anchor_generator.base_sizes = [4, 9, 17, 31, 64]\n# cfg.model.rpn_head.anchor_generator.strides = [4, 8, 16, 32, 64]\n\ncfg.model.train_cfg.rpn.sampler.num = 1024\ncfg.model.train_cfg.rpn_proposal.nms_post = 2000\nfor rcnn in cfg.model.train_cfg.rcnn:\n    rcnn.sampler.num = 3072\ncfg.model.test_cfg.rpn.nms_pre = 3000\ncfg.model.test_cfg.rpn.nms_post = 3000\n# cfg.model.test_cfg.rpn.max_num = 3000\n#edits to train and test cfg are from https://github.com/Media-Smart/SKU110K-DenseDet/blob/master/configs/SKU_fusion_bfp_x101_32x4d.py\n\ncfg.seed = 0\nset_random_seed(0, deterministic=False)\ncfg.gpu_ids = range(1)\ncfg.fp16 = dict(loss_scale=512.0)\nmeta = dict()\nmeta['config'] = cfg.pretty_text\n\n\n\nprint(f'Config:\\n{cfg.pretty_text}')","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2022-11-12T01:33:03.834957Z","iopub.execute_input":"2022-11-12T01:33:03.835288Z","iopub.status.idle":"2022-11-12T01:33:05.456512Z","shell.execute_reply.started":"2022-11-12T01:33:03.835255Z","shell.execute_reply":"2022-11-12T01:33:05.455456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if MyConfig.make_training:\n    datasets = [build_dataset(cfg.data.train)]\n    model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n    model.CLASSES = datasets[0].CLASSES\n\n    mmcv.mkdir_or_exist(os.path.abspath(cfg.work_dir))\n    train_detector(model, datasets, cfg, distributed=False, validate=True, meta=meta)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T00:26:02.611638Z","iopub.execute_input":"2022-11-12T00:26:02.612015Z","iopub.status.idle":"2022-11-12T00:26:02.618604Z","shell.execute_reply.started":"2022-11-12T00:26:02.611983Z","shell.execute_reply":"2022-11-12T00:26:02.617357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inference with saved model weights:","metadata":{}},{"cell_type":"code","source":"if MyConfig.make_inference:\n    checkpoint_file = '../../input/crcnn-human-silhouettes-in-fores/model_output/epoch_50.pth'\n    model = init_detector(cfg, checkpoint_file, device='cuda:0')","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:33:05.458128Z","iopub.execute_input":"2022-11-12T01:33:05.458544Z","iopub.status.idle":"2022-11-12T01:33:18.217511Z","shell.execute_reply.started":"2022-11-12T01:33:05.458505Z","shell.execute_reply":"2022-11-12T01:33:18.216189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_part[val_part.count_region > 0]","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:35:44.996185Z","iopub.execute_input":"2022-11-12T01:35:44.996845Z","iopub.status.idle":"2022-11-12T01:35:45.015132Z","shell.execute_reply.started":"2022-11-12T01:35:44.996805Z","shell.execute_reply":"2022-11-12T01:35:45.013996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_predict(img_path, thrh=0.2, circles=True, put_score=True):\n\n    img = mmcv.imread(img_path)\n    result = inference_detector(model, img)\n\n    coords_arr = result[0]\n    \n    for item in coords_arr:\n        if item[4] > thrh:\n            cx = int((item[2] + item[0]) // 2)\n            cy = int((item[3] + item[1]) // 2)\n            r = int(item[3] - item[1]) // 2\n\n            w = int((item[2] - item[0]))\n            h = int((item[3] - item[1]))\n\n            x1 = int(item[0])\n            y1 = int(item[1])\n            \n            if circles:\n                img = cv2.circle(img, (cx, cy), r, (255,0,0), 2)\n            else:\n                img = cv2.rectangle(img, (x1, y1), (x1+w, y1+h), (255,0,0), 2)\n            if put_score:\n                img = cv2.putText(img, str(item[4]), (int(item[0]), int(item[1])), cv2.FONT_HERSHEY_SIMPLEX, 1.5, 255, 2)\n\n    plt.figure(figsize=(30, 20))\n    plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:35:49.166230Z","iopub.execute_input":"2022-11-12T01:35:49.167397Z","iopub.status.idle":"2022-11-12T01:35:49.178141Z","shell.execute_reply.started":"2022-11-12T01:35:49.167349Z","shell.execute_reply":"2022-11-12T01:35:49.176864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize_predict('../../input/detection-of-human-silhouettes-in-forest-images/test_dataset_test/test/890.JPG', thrh = 0.45, circles = True, put_score=True)\nvisualize_predict('../../input/detection-of-human-silhouettes-in-forest-images/train_dataset_train/train/3835.JPG', thrh = 0.45, circles = True, put_score=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:35:51.667219Z","iopub.execute_input":"2022-11-12T01:35:51.667968Z","iopub.status.idle":"2022-11-12T01:35:57.271721Z","shell.execute_reply.started":"2022-11-12T01:35:51.667930Z","shell.execute_reply":"2022-11-12T01:35:57.270384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_subm = pd.read_csv('../../input/detection-of-human-silhouettes-in-forest-images/omsk/sample_solution.csv')\nsample_subm['region_shape'] = sample_subm['region_shape'].astype('object')\nsample_subm.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:36:05.185904Z","iopub.execute_input":"2022-11-12T01:36:05.186274Z","iopub.status.idle":"2022-11-12T01:36:05.206840Z","shell.execute_reply.started":"2022-11-12T01:36:05.186241Z","shell.execute_reply":"2022-11-12T01:36:05.205971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\ntest_image_dir = '../../input/detection-of-human-silhouettes-in-forest-images/test_dataset_test/test'\nboxes_count_all = 0\n\nthrh = 0.3\n\nmmdet_boxes_pred = {'img': [], 'Xmin': [], 'Ymin':[], 'Xmax':[], 'Ymax':[], 'Conf':[], 'img_h': [], 'img_w': []}\n\nfor i, row in tqdm(sample_subm.iterrows(), total=sample_subm.shape[0]):\n    img = mmcv.imread(f'{test_image_dir}/{row.ID_img}')\n    result = inference_detector(model, img)\n    \n    predict_region_shape = ''\n    cxs, cys, rs = [], [], []\n    \n    for box in result[0]:\n        if box[4] > thrh:\n            mmdet_boxes_pred['img'].append(row.ID_img)\n            mmdet_boxes_pred['Xmin'].append(box[0])\n            mmdet_boxes_pred['Ymin'].append(box[1])\n            mmdet_boxes_pred['Xmax'].append(box[2])\n            mmdet_boxes_pred['Ymax'].append(box[3])\n            mmdet_boxes_pred['Conf'].append(box[4])\n            mmdet_boxes_pred['img_h'].append(img.shape[0])\n            mmdet_boxes_pred['img_w'].append(img.shape[1])\n\n            cx = int((box[2] + box[0]) // 2)\n            cy = int((box[3] + box[1]) // 2)\n            r = int(box[3] - box[1]) // 2 #             r = int((int(box[3] - box[1]) // 2) // 1.5)\n            \n            boxes_count_all += 1\n            cxs.append(cx)\n            cys.append(cy)\n            rs.append(r)\n    if len(rs) > 0:\n        circles = sorted(zip(cxs, cys,rs))\n        arr_coords = [f'{{\"cx\":{cx}, \"cy\":{cy}, \"r\":{r}}}' for (cx, cy, rs) in circles]\n        sample_subm.at[i, 'region_shape'] = arr_coords\n    else:\n        sample_subm.at[i, 'region_shape'] = 0","metadata":{"execution":{"iopub.status.busy":"2022-11-12T01:36:07.006073Z","iopub.execute_input":"2022-11-12T01:36:07.006464Z","iopub.status.idle":"2022-11-12T02:25:41.617372Z","shell.execute_reply.started":"2022-11-12T01:36:07.006428Z","shell.execute_reply":"2022-11-12T02:25:41.616375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predicts_crcnn = pd.DataFrame({'Xmin': mmdet_boxes_pred['Xmin'], 'Ymin': mmdet_boxes_pred['Ymin'],\\\n                  'Xmax': mmdet_boxes_pred['Xmax'], 'Ymax': mmdet_boxes_pred['Ymax'],\\\n                  'ID_image': mmdet_boxes_pred['img'], 'Conf': mmdet_boxes_pred['Conf'], 'h':mmdet_boxes_pred['img_h'], 'w': mmdet_boxes_pred['img_w']})\n\n\ntest_predicts_crcnn['Ymin'] = test_predicts_crcnn['Ymin'] / test_predicts_crcnn['h']\ntest_predicts_crcnn['Ymax'] = test_predicts_crcnn['Ymax'] / test_predicts_crcnn['h']\ntest_predicts_crcnn['Xmin'] = test_predicts_crcnn['Xmin'] / test_predicts_crcnn['w']\ntest_predicts_crcnn['Xmax'] = test_predicts_crcnn['Xmax'] / test_predicts_crcnn['w']\ntest_predicts_crcnn['Ymin'] = test_predicts_crcnn['Ymin']\ntest_predicts_crcnn['Ymax'] = test_predicts_crcnn['Ymax']\ntest_predicts_crcnn['Xmin'] = test_predicts_crcnn['Xmin']\ntest_predicts_crcnn['Xmax'] = test_predicts_crcnn['Xmax']\ntest_predicts_crcnn['label'] = 0","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:51:13.209830Z","iopub.execute_input":"2022-11-12T02:51:13.210188Z","iopub.status.idle":"2022-11-12T02:51:13.224119Z","shell.execute_reply.started":"2022-11-12T02:51:13.210158Z","shell.execute_reply":"2022-11-12T02:51:13.222899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predicts_crcnn = test_predicts_crcnn[test_predicts_crcnn.Conf > 0.55]\nlen(test_predicts_crcnn['ID_image'].unique()), len(test_predicts_crcnn)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:05:49.819038Z","iopub.execute_input":"2022-11-12T03:05:49.819431Z","iopub.status.idle":"2022-11-12T03:05:49.828922Z","shell.execute_reply.started":"2022-11-12T03:05:49.819398Z","shell.execute_reply":"2022-11-12T03:05:49.827965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(sample_subm[sample_subm.region_shape != 0]), boxes_count_all","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:51:29.382186Z","iopub.execute_input":"2022-11-12T02:51:29.383291Z","iopub.status.idle":"2022-11-12T02:51:29.391905Z","shell.execute_reply.started":"2022-11-12T02:51:29.383235Z","shell.execute_reply":"2022-11-12T02:51:29.390836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_subm[sample_subm.region_shape != 0].head(15)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:37:38.769911Z","iopub.execute_input":"2022-11-12T02:37:38.770617Z","iopub.status.idle":"2022-11-12T02:37:38.786272Z","shell.execute_reply.started":"2022-11-12T02:37:38.770573Z","shell.execute_reply":"2022-11-12T02:37:38.785100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_subm.to_csv('../crcnn_60ep_0_3.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:32:34.745275Z","iopub.execute_input":"2022-11-12T02:32:34.745658Z","iopub.status.idle":"2022-11-12T02:32:34.759076Z","shell.execute_reply.started":"2022-11-12T02:32:34.745624Z","shell.execute_reply":"2022-11-12T02:32:34.758085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_predict('../../input/detection-of-human-silhouettes-in-forest-images/test_dataset_test/test/337.jpg', thrh = 0.3, circles = True, put_score=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:32:36.205200Z","iopub.execute_input":"2022-11-12T02:32:36.205910Z","iopub.status.idle":"2022-11-12T02:32:41.422857Z","shell.execute_reply.started":"2022-11-12T02:32:36.205876Z","shell.execute_reply":"2022-11-12T02:32:41.421377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# YOLOv5","metadata":{}},{"cell_type":"code","source":"%cd ..","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:09:30.346791Z","iopub.execute_input":"2022-11-12T03:09:30.347156Z","iopub.status.idle":"2022-11-12T03:09:30.354425Z","shell.execute_reply.started":"2022-11-12T03:09:30.347125Z","shell.execute_reply":"2022-11-12T03:09:30.353246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_dataset_file():\n    category_names = ['human']\n    category_names = {k: v for v, k in enumerate(category_names)}\n    names_str = \" \\n \".join([f'{item[1]}: {item[0]}' for item in list(zip(category_names.keys(), category_names.values()))])\n    yaml_content = f\"\"\"\n    train: /kaggle/working/data/train/images\n    val: /kaggle/working/data/val/images\n\n    # number of classes\n    nc: 1\n\n    # class names\n    names: \\n {names_str}\n    \"\"\"\n\n    with open('./data/dataset.yaml', 'w') as f:\n        f.write(yaml_content)\n        \nmake_dataset_file()\n!cat ./data/dataset.yaml","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:09:34.758885Z","iopub.execute_input":"2022-11-12T03:09:34.759813Z","iopub.status.idle":"2022-11-12T03:09:34.954774Z","shell.execute_reply.started":"2022-11-12T03:09:34.759766Z","shell.execute_reply":"2022-11-12T03:09:34.953532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5.git\n!pip install -r ./yolov5/requirements.txt","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-11-12T03:09:38.423232Z","iopub.execute_input":"2022-11-12T03:09:38.424273Z","iopub.status.idle":"2022-11-12T03:09:53.466701Z","shell.execute_reply.started":"2022-11-12T03:09:38.424232Z","shell.execute_reply":"2022-11-12T03:09:53.465491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# YOLOv5l YOLOv5x YOLOv5s6 YOLOv5m6 YOLOv5l6 YOLOv5x6\n# !cd yolov5 && python train.py --img 1280 --batch 16 --epochs 30 --data ../data/dataset.yaml --weights yolov5s.pt\n# !cd yolov5 && python -m torch.distributed.run --nproc_per_node 2 train.py --img 1920 --batch 4 --epochs 90 --data ../data/dataset.yaml --weights yolov5l.pt --hyp hyp.scratch-med.yaml --device 0,1 --cache disk\n\n# !cd yolov5 && python train.py --img 1920 --batch 16 --epochs 20 --data ../data/dataset.yaml --weights yolov5s.pt --hyp hyp.scratch-med.yaml","metadata":{"execution":{"iopub.status.busy":"2022-11-11T19:17:12.622484Z","iopub.execute_input":"2022-11-11T19:17:12.622830Z","iopub.status.idle":"2022-11-11T19:17:12.629005Z","shell.execute_reply.started":"2022-11-11T19:17:12.622794Z","shell.execute_reply":"2022-11-11T19:17:12.627894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class YOLOv5(Enum):\n    yolov5s = 'yolov5s.pt'\n    yolov5m = 'yolov5m.pt'\n    yolov5l = 'yolov5l.pt'\n\nclass Yolo_hyp(Enum):\n    low = 'hyp.scratch-low.yaml'\n    med = 'hyp.scratch-med.yaml'\n    high = 'hyp.scratch-high.yaml'\n    \nclass YoloTrainingConfig:\n    resolution = 1920\n    batch_size = 6\n    epochs = 90\n    dataset_path = '../data/dataset.yaml'\n    weights = YOLOv5.yolov5m.value\n    hyp = Yolo_hyp.med.value","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:09:53.469367Z","iopub.execute_input":"2022-11-12T03:09:53.470109Z","iopub.status.idle":"2022-11-12T03:09:53.478592Z","shell.execute_reply.started":"2022-11-12T03:09:53.470066Z","shell.execute_reply":"2022-11-12T03:09:53.477463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if MyConfig.make_training:\n    !cd yolov5 && python -m torch.distributed.run --nproc_per_node 2 train.py --img $YoloTrainingConfig.resolution --batch $YoloTrainingConfig.batch_size \\\n    --epochs $YoloTrainingConfig.epochs --data ../data/dataset.yaml --weights $YoloTrainingConfig.weights --hyp $YoloTrainingConfig.hyp --device 0,1 --cache disk","metadata":{"execution":{"iopub.status.busy":"2022-11-11T23:16:22.502029Z","iopub.execute_input":"2022-11-11T23:16:22.502792Z","iopub.status.idle":"2022-11-11T23:16:22.518081Z","shell.execute_reply.started":"2022-11-11T23:16:22.502756Z","shell.execute_reply":"2022-11-11T23:16:22.517090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yolov5 inference config","metadata":{}},{"cell_type":"code","source":"class YoloInferenceConfig:\n        \n    yolov5_checkpoint_path = '../../input/detection-of-human-silhouettes-in-forest-weights/best_m_1920_90ep.pt'\n    resolution = 1920\n    model_arch = 'yolov5m'\n    test_images_path = '../input/detection-of-human-silhouettes-in-forest-images/test_dataset_test/test'\n    test_csv_path = ''\n    sample_subm_path = '../input/detection-of-human-silhouettes-in-forest-images/omsk/sample_solution.csv'\n    iou = 0.45\n    thresh = 0.25\n    agnostic = '--agnostic'\n    augment = '--augment'\n    half = '--half' # --half\n    from_saved_weights = True\n    source_path = f'../{MyConfig.test_dir}'","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:10:23.268960Z","iopub.execute_input":"2022-11-12T03:10:23.269353Z","iopub.status.idle":"2022-11-12T03:10:23.274691Z","shell.execute_reply.started":"2022-11-12T03:10:23.269293Z","shell.execute_reply":"2022-11-12T03:10:23.273541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# last run\nif MyConfig.make_training:\n    exp_last = sorted(next(os.walk('./yolov5/runs/train/'))[1])[-1]\n    !echo $exp_last","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:10:26.455082Z","iopub.execute_input":"2022-11-12T03:10:26.455877Z","iopub.status.idle":"2022-11-12T03:10:26.462340Z","shell.execute_reply.started":"2022-11-12T03:10:26.455830Z","shell.execute_reply":"2022-11-12T03:10:26.461302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if MyConfig.make_inference:\n    !cd yolov5 && python detect.py $YoloInferenceConfig.augment --img $YoloInferenceConfig.resolution --conf $YoloInferenceConfig.thresh \\\n    --iou $YoloInferenceConfig.iou --source $YoloInferenceConfig.source_path --weights $YoloInferenceConfig.yolov5_checkpoint_path --save-txt --save-conf $YoloInferenceConfig.half","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-11-12T03:10:28.251127Z","iopub.execute_input":"2022-11-12T03:10:28.251510Z","iopub.status.idle":"2022-11-12T03:21:04.817965Z","shell.execute_reply.started":"2022-11-12T03:10:28.251479Z","shell.execute_reply":"2022-11-12T03:21:04.816744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if MyConfig.make_inference:\n    exp_detect_last = sorted(next(os.walk('./yolov5/runs/detect/'))[1])[-1]","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:21:43.518563Z","iopub.execute_input":"2022-11-12T03:21:43.518937Z","iopub.status.idle":"2022-11-12T03:21:43.524703Z","shell.execute_reply.started":"2022-11-12T03:21:43.518907Z","shell.execute_reply":"2022-11-12T03:21:43.523625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class YoloInferenceConfig():\n    \n#     def __init__(self):\n#         self._agnostic  = True\n    \n#     @property\n#     def agnostic(self):\n#         if self._agnostic:\n#             return '--agnostic'\n#         else:\n#             return ''\n        \n#     @agnostic.setter\n#     def agnostic(self, value):\n#         self._agnostic = value\n\n# YoloConfig = YoloInferenceConfig()\n# YoloConfig.agnostic","metadata":{"execution":{"iopub.status.busy":"2022-11-11T19:36:49.141331Z","iopub.execute_input":"2022-11-11T19:36:49.141982Z","iopub.status.idle":"2022-11-11T19:36:49.149740Z","shell.execute_reply.started":"2022-11-11T19:36:49.141947Z","shell.execute_reply":"2022-11-11T19:36:49.148629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_solution_labels_df(path_to_txt_folder, sample_subm):\n    simple_solution = []\n    for detection_file in os.listdir(path_to_txt_folder):\n        img_name = sample_subm[sample_subm['ID_img'].str.contains(detection_file.split('.')[0])]['ID_img'].iloc[0]\n        with open(path_to_txt_folder + detection_file, 'r') as f:\n            data = f.read()\n            data = [i for i in data.split('\\n') if i != '']\n        for line in data:\n            val = [float(i) for i in line.split()]\n            cls, xywh, conf = val[0], val[1:5], val[5]\n            center_x, center_y, width, height = xywh\n            xmin = center_x - (width / 2)\n            xmax = center_x + (width / 2)\n            ymin = center_y - (height / 2)\n            ymax = center_y + (height / 2)\n            simple_solution.append([img_name, cls, conf, xmin, xmax, ymin, ymax, center_x, center_y, width, height])\n    return simple_solution","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:21:47.110617Z","iopub.execute_input":"2022-11-12T03:21:47.110989Z","iopub.status.idle":"2022-11-12T03:21:47.120787Z","shell.execute_reply.started":"2022-11-12T03:21:47.110956Z","shell.execute_reply":"2022-11-12T03:21:47.118728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predicts = get_solution_labels_df(f'./yolov5/runs/detect/{exp_detect_last}/labels/', MyConfig.sample_subm)\ntest_predicts = pd.DataFrame(test_predicts, columns=['ID_image', 'label', 'Conf', 'XMin', 'XMax', 'YMin', 'YMax', 'center_x', 'center_y', 'width', 'height'])","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:21:50.122790Z","iopub.execute_input":"2022-11-12T03:21:50.123566Z","iopub.status.idle":"2022-11-12T03:21:50.368186Z","shell.execute_reply.started":"2022-11-12T03:21:50.123527Z","shell.execute_reply":"2022-11-12T03:21:50.367189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predicts = test_predicts[test_predicts.Conf > 0.44]\ntest_predicts.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:21:52.656819Z","iopub.execute_input":"2022-11-12T03:21:52.659578Z","iopub.status.idle":"2022-11-12T03:21:52.685943Z","shell.execute_reply.started":"2022-11-12T03:21:52.659536Z","shell.execute_reply":"2022-11-12T03:21:52.685092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_predicts), len(test_predicts['ID_image'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:21:54.906736Z","iopub.execute_input":"2022-11-12T03:21:54.907652Z","iopub.status.idle":"2022-11-12T03:21:54.917157Z","shell.execute_reply.started":"2022-11-12T03:21:54.907610Z","shell.execute_reply":"2022-11-12T03:21:54.916230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detect_img_path = f'./yolov5/runs/detect/{exp_detect_last}'\nuniq_imgs_list = test_predicts['ID_image'].unique()\nimg_path = f'{detect_img_path}/{uniq_imgs_list[5]}'\nimg = cv2.imread(img_path)\nplt.figure(figsize=(30, 20))\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:21:58.254507Z","iopub.execute_input":"2022-11-12T03:21:58.254886Z","iopub.status.idle":"2022-11-12T03:22:02.370118Z","shell.execute_reply.started":"2022-11-12T03:21:58.254854Z","shell.execute_reply":"2022-11-12T03:22:02.367410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## WBF","metadata":{}},{"cell_type":"code","source":"# https://github.com/ZFTurbo/Weighted-Boxes-Fusion\n\n!pip install ensemble-boxes","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:38:07.246465Z","iopub.execute_input":"2022-11-12T02:38:07.247163Z","iopub.status.idle":"2022-11-12T02:38:17.749667Z","shell.execute_reply.started":"2022-11-12T02:38:07.247127Z","shell.execute_reply":"2022-11-12T02:38:17.748390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from ensemble_boxes import *\n\n# boxes_list = test_predicts[['XMin', 'YMin', 'XMax', 'YMax']].values\n\n# scores_list = test_predicts['Conf'].values\n# labels_list = test_predicts['label'].values\n# # weights = [2, 1]\n\n# iou_thr = 0.5\n# skip_box_thr = 0.0001\n# sigma = 0.1\n# thresh = 0.44\n\n# # Merge boxes for single model predictions\n# boxes, scores, labels = weighted_boxes_fusion([boxes_list], [scores_list], [labels_list], weights=None, iou_thr=iou_thr)\n\n# boxes, scores, labels = nms(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr)\n# boxes, scores, labels = soft_nms(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr, sigma=sigma, thresh=skip_box_thr)\n# boxes, scores, labels = non_maximum_weighted(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n# boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)","metadata":{"execution":{"iopub.status.busy":"2022-11-11T19:45:14.260671Z","iopub.execute_input":"2022-11-11T19:45:14.261069Z","iopub.status.idle":"2022-11-11T19:45:14.283532Z","shell.execute_reply.started":"2022-11-11T19:45:14.261037Z","shell.execute_reply":"2022-11-11T19:45:14.282228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def show_image(im):\n#     plt.imshow(im.astype(np.uint8))\n    \n# def gen_color_list(model_num, labels_num):\n#     color_list = np.zeros((model_num, labels_num, 3))\n#     colors_to_use = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (0, 255, 255), (255, 0, 255), (255, 255, 0), (0, 0, 0)]\n#     total = 0\n#     for i in range(model_num):\n#         for j in range(labels_num):\n#             color_list[i, j, :] = colors_to_use[total]\n#             total = (total + 1) % len(colors_to_use)\n#     return color_list\n\n# def show_boxes(boxes_list, scores_list, labels_list, image_size=800):\n#     thickness = 5\n#     color_list = gen_color_list(len(boxes_list), len(np.unique(labels_list)))\n#     image = np.zeros((image_size, image_size, 3), dtype=np.uint8)\n#     image[...] = 255\n#     for i in range(len(boxes_list)):\n#         for j in range(len(boxes_list[i])):\n#             x1 = int(image_size * boxes_list[i][j][0])\n#             y1 = int(image_size * boxes_list[i][j][1])\n#             x2 = int(image_size * boxes_list[i][j][2])\n#             y2 = int(image_size * boxes_list[i][j][3])\n#             lbl = labels_list[i][j]\n#             cv2.rectangle(image, (x1, y1), (x2, y2), color_list[i][lbl], int(thickness * scores_list[i][j]))\n#     show_image(image)","metadata":{"execution":{"iopub.status.busy":"2022-11-11T19:48:04.753695Z","iopub.execute_input":"2022-11-11T19:48:04.754660Z","iopub.status.idle":"2022-11-11T19:48:04.766181Z","shell.execute_reply.started":"2022-11-11T19:48:04.754622Z","shell.execute_reply":"2022-11-11T19:48:04.765023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  show_boxes([boxes], [scores], [labels.astype(np.int32)])","metadata":{"execution":{"iopub.status.busy":"2022-11-11T19:47:46.893592Z","iopub.execute_input":"2022-11-11T19:47:46.894522Z","iopub.status.idle":"2022-11-11T19:47:47.195801Z","shell.execute_reply.started":"2022-11-11T19:47:46.894486Z","shell.execute_reply":"2022-11-11T19:47:47.194644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### WBF","metadata":{}},{"cell_type":"code","source":"from ensemble_boxes import *\n\ndef wbf_process(df, coord_cols: List[str]):\n    \n    iou_thr = 0.2 # 0.2: 0.59850\n    skip_box_thr = 0.44 # 0.44\n#     sigma = 0.1\n    wbf_results = {'boxes':[], 'scores':[], 'labels':[], 'ID_image':[]}\n    \n    for name, group in df.groupby(['ID_image']):\n        boxes_list = df.loc[group.index, coord_cols].values\n        scores_list = df.loc[group.index, 'Conf'].values\n        labels_list = df.loc[group.index, 'label'].values\n        boxes, scores, labels = weighted_boxes_fusion([boxes_list], [scores_list], [labels_list], weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n        wbf_results['boxes'].append(boxes)\n        wbf_results['scores'].append(scores)\n        wbf_results['labels'].append(labels)\n        wbf_results['ID_image'].append(df.loc[group.index, 'ID_image'].values[0])\n\n        if len(df.loc[group.index, 'ID_image']) != len(boxes):\n            print(len(boxes))\n\n    boxes_after_wbf = []\n    lmgs_after_wbf = []\n    scores_after_wbf = []\n\n    for img_predict in zip(wbf_results['boxes'], wbf_results['ID_image'], wbf_results['scores']):\n        for item in zip(img_predict[0], img_predict[2]):\n            boxes_after_wbf.append(item[0])\n            lmgs_after_wbf.append(img_predict[1])\n            scores_after_wbf.append(item[1])\n\n    test_predicts_ = pd.DataFrame({'Xmin': np.array(boxes_after_wbf)[:, 0], 'Ymin': np.array(boxes_after_wbf)[:, 1],\\\n                  'Xmax': np.array(boxes_after_wbf)[:, 2], 'Ymax': np.array(boxes_after_wbf)[:, 3],\\\n                  'ID_image': lmgs_after_wbf, 'Conf': scores_after_wbf})\n    return test_predicts_\n\ntest_predicts_ = wbf_process(test_predicts, ['XMin', 'YMin', 'XMax', 'YMax'])\ntest_predicts_crcnn_wbf = wbf_process(test_predicts_crcnn, ['Xmin', 'Ymin', 'Xmax', 'Ymax'])","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:23:15.065267Z","iopub.execute_input":"2022-11-12T03:23:15.065966Z","iopub.status.idle":"2022-11-12T03:23:15.418247Z","shell.execute_reply.started":"2022-11-12T03:23:15.065932Z","shell.execute_reply":"2022-11-12T03:23:15.417165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_predicts_crcnn_wbf), len(test_predicts_crcnn_wbf['ID_image'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:23:18.258892Z","iopub.execute_input":"2022-11-12T03:23:18.259262Z","iopub.status.idle":"2022-11-12T03:23:18.267085Z","shell.execute_reply.started":"2022-11-12T03:23:18.259226Z","shell.execute_reply":"2022-11-12T03:23:18.266017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_predicts_), len(test_predicts_['ID_image'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:23:21.003099Z","iopub.execute_input":"2022-11-12T03:23:21.003803Z","iopub.status.idle":"2022-11-12T03:23:21.014286Z","shell.execute_reply.started":"2022-11-12T03:23:21.003765Z","shell.execute_reply":"2022-11-12T03:23:21.013321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_c_h_w(df: pd.DataFrame) -> None:\n    df['center_x'] = (df['Xmin'] + df['Xmax']) / 2\n    df['center_y'] = (df['Ymin'] + df['Ymax']) / 2\n    df['r'] = (df['Xmax'] - df['Xmin']) / 2\n    df['width'] = df['Xmax'] - df['Xmin']\n    df['height'] = df['Ymax'] - df['Ymin']\n    \nadd_c_h_w(test_predicts_)\nadd_c_h_w(test_predicts_crcnn_wbf)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:23:23.314996Z","iopub.execute_input":"2022-11-12T03:23:23.315389Z","iopub.status.idle":"2022-11-12T03:23:23.329673Z","shell.execute_reply.started":"2022-11-12T03:23:23.315354Z","shell.execute_reply":"2022-11-12T03:23:23.328647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def denorm_coords(df: pd.DataFrame) -> None:\n    for i, row in df.iterrows():\n        img = cv2.imread(f'{MyConfig.test_dir}/{row[\"ID_image\"]}')\n        h, w = img.shape[0], img.shape[1]\n        center_x = row['center_x'] * w\n        center_y = row['center_y'] * h\n        width = row['width'] * w\n        height = row['height'] * h\n        r = width / 2\n        \n        df.at[i,'cx'] = int(center_x)\n        df.at[i,'cy'] = int(center_y)\n        df.at[i,'r'] = int(r)\n    \n    df['cx'] = df['cx'].astype(int)\n    df['cy'] = df['cy'].astype(int)\n    df['r'] = df['r'].astype(int)\n        \ndenorm_coords(test_predicts_)\ndenorm_coords(test_predicts_crcnn_wbf)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:23:25.474917Z","iopub.execute_input":"2022-11-12T03:23:25.475277Z","iopub.status.idle":"2022-11-12T03:24:34.065605Z","shell.execute_reply.started":"2022-11-12T03:23:25.475246Z","shell.execute_reply":"2022-11-12T03:24:34.064593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_subm_format(df: pd.DataFrame, sample_subm) -> None:\n    for i, row in MyConfig.sample_subm.iterrows():\n        if len(df[df.ID_image == row[\"ID_img\"]]) > 0:\n            circle_coords = [f'{{\"cx\":{item[1][\"cx\"]}, \"cy\":{item[1][\"cy\"]}, \"r\":{int(item[1][\"r\"])}}}' for item in df[df.ID_image == row['ID_img']].sort_values(by=['cx', 'cy']).iterrows()]\n            sample_subm.at[i, 'region_shape'] = circle_coords\n        else:\n            sample_subm.at[i, 'region_shape'] = 0\n            \nmake_subm_format(test_predicts_, MyConfig.sample_subm)\n\ncrnn_sample_sumb = MyConfig.sample_subm.copy()\n\nmake_subm_format(test_predicts_crcnn_wbf, crnn_sample_sumb)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:24:34.067716Z","iopub.execute_input":"2022-11-12T03:24:34.068120Z","iopub.status.idle":"2022-11-12T03:24:37.152294Z","shell.execute_reply.started":"2022-11-12T03:24:34.068083Z","shell.execute_reply":"2022-11-12T03:24:37.151324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MyConfig.sample_subm[MyConfig.sample_subm.region_shape != 0].head(20)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:24:37.153737Z","iopub.execute_input":"2022-11-12T03:24:37.154108Z","iopub.status.idle":"2022-11-12T03:24:37.171176Z","shell.execute_reply.started":"2022-11-12T03:24:37.154071Z","shell.execute_reply":"2022-11-12T03:24:37.169988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crnn_sample_sumb[crnn_sample_sumb.region_shape != 0].head(20)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:24:37.173885Z","iopub.execute_input":"2022-11-12T03:24:37.174761Z","iopub.status.idle":"2022-11-12T03:24:37.191612Z","shell.execute_reply.started":"2022-11-12T03:24:37.174716Z","shell.execute_reply":"2022-11-12T03:24:37.190391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vis_img_with_pred(path: str, sample_subm: pd.DataFrame):\n    \n    img = cv2.imread(path)\n    coords_str = str(sample_subm.iloc[index]['region_shape']).replace(\"'\", \"\")\n    circle_coords = json.loads(coords_str)\n    yolo_coords_f = ''\n    for item in circle_coords:\n        cx = item['cx']\n        cy = item['cy']\n        r = int(item['r'] // 2) # old: 1.5  r = item['r']\n        \n        print(cx, cy, r)\n   \n        img = cv2.circle(img, (cx, cy), r, (255,0,0), 2)\n\n    plt.figure(figsize=(30, 20))\n    plt.imshow(img)\n    \nindex = 503\nimg_path = f'{MyConfig.test_dir}/{MyConfig.sample_subm.iloc[index][\"ID_img\"]}'\nvis_img_with_pred(img_path, sample_subm)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:24:37.193320Z","iopub.execute_input":"2022-11-12T03:24:37.193769Z","iopub.status.idle":"2022-11-12T03:24:41.383387Z","shell.execute_reply.started":"2022-11-12T03:24:37.193732Z","shell.execute_reply":"2022-11-12T03:24:41.382257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_subm_file_name = './yolov5m_1920_90ep_0_44_wbf.csv'\ncsv_subm_file_name_crnn = './crcnn_wbf.csv'\nMyConfig.sample_subm.to_csv(csv_subm_file_name, index=False)\ncrnn_sample_sumb.to_csv(csv_subm_file_name_crnn, index=False)\n\ndisplay(FileLink(csv_subm_file_name))\nFileLink(csv_subm_file_name_crnn)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T03:38:17.365953Z","iopub.execute_input":"2022-11-12T03:38:17.366451Z","iopub.status.idle":"2022-11-12T03:38:17.400101Z","shell.execute_reply.started":"2022-11-12T03:38:17.366409Z","shell.execute_reply":"2022-11-12T03:38:17.399171Z"},"trusted":true},"execution_count":220,"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/yolov5m_1920_90ep_0_44_wbf.csv","text/html":"<a href='./yolov5m_1920_90ep_0_44_wbf.csv' target='_blank'>./yolov5m_1920_90ep_0_44_wbf.csv</a><br>"},"metadata":{}},{"execution_count":220,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/crcnn_wbf.csv","text/html":"<a href='./crcnn_wbf.csv' target='_blank'>./crcnn_wbf.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}